{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist_classes import get_mnist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def Dtanh(A):\n",
    "    return 1 - np.multiply(A,A)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def Dsigmoid(A):\n",
    "    return np.multiply(A,1 - A)\n",
    "\n",
    "def relu(z,leak=0):\n",
    "    assert (0 <= leak) and (leak < 0.5)\n",
    "    return np.max(leak * Z,Z)\n",
    "\n",
    "def Drelu(A):\n",
    "    t1 = (A > 0) * 1\n",
    "    t2 = (A <= 0) * leak\n",
    "    return t1 + t2\n",
    "\n",
    "def identity(Z):\n",
    "    return Z\n",
    "\n",
    "def Didentity(A):\n",
    "    return np.ones(A.shape)\n",
    "\n",
    "def softmax(Z):\n",
    "    #print('From within softmax:')\n",
    "    #print('Type of input array Z:',type(Z))\n",
    "    return np.exp(Z) / np.sum(np.exp(Z),axis=1,keepdims=True)\n",
    "\n",
    "def softmaxLoss(P,Y):\n",
    "    return -np.mean(np.sum(np.multiply(Y,np.log(P)),axis=1))\n",
    "\n",
    "def sigmoidLoss(P,Y):\n",
    "    return -np.mean(np.sum(np.multiply(Y,np.log(P)) + np.multiply((1 - Y),np.log(1 - P)),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FcLayer(object):\n",
    "    '''Object class representing a fully connected layer in a feed-forward neural network'''\n",
    "    def __init__(self,n,activation='tanh'):\n",
    "        self.sizeIn = None\n",
    "        self.sizeOut = [n]\n",
    "        \n",
    "        assert activation in ['tanh','sigmoid','relu','identity','softmax']\n",
    "        if activation == 'tanh':\n",
    "            self.activation = (tanh,activation)\n",
    "            self.Dactivation = Dtanh\n",
    "        elif activation == 'sigmoid': # possibly output layer, attach loss function just in case\n",
    "            self.activation = (sigmoid,activation)\n",
    "            self.Dactivation = Dsigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activation = (relu,activation)\n",
    "            self.Dactivation = Drelu\n",
    "        elif activation == 'identity':\n",
    "            self.activation = (identity,activation)\n",
    "            self.Dactivation = Didentity\n",
    "        elif activation == 'softmax': # definitely output layer, so no need for Dactivation\n",
    "            self.activation = (softmax,activation)\n",
    "            \n",
    "        self.Weight = None\n",
    "        self.bias = None\n",
    "        self.cache = {'A':None,'DZ':None}\n",
    "        self.previousLayer = None\n",
    "        self.nextLayer = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns a string with bio of layer.'''\n",
    "\n",
    "        bio = '----------------------------------------' \\\n",
    "                + '\\n Layer type: Fully connected layer' \\\n",
    "                + '\\n Number of neurons in input data: ' + str(self.sizeIn) \\\n",
    "                + '\\n Type of activation used: ' + self.activation[1] \\\n",
    "                + '\\n Number of neurons in output data: ' + str(self.sizeOut) \\\n",
    "            \n",
    "        return bio\n",
    "        \n",
    "    def forwardProp(self):\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        Z_c = np.dot(A_p,self.Weight) + self.bias\n",
    "        #print(\"From within fully connected layer's forwardProp:\")\n",
    "        #print(\"Type of Z_c:\", type(Z_c))\n",
    "        A_c = self.activation[0](Z_c)\n",
    "        #print(\"Type of A_c:\", type(A_c))\n",
    "        #print(\"From within fully connected layer's forwardProp:\")\n",
    "        #print(\"Shape of previous layer's activation:\",A_p.shape)\n",
    "        #print(\"Shape of current layer's activation:\",A_c.shape)\n",
    "        #print(\"------------------------------------\")\n",
    "        self.cache['A'] = A_c\n",
    "        \n",
    "    def getDZ_c(self,DA_c):\n",
    "        # calculates this layer's DZ. gets called from next layer in network during backprop\n",
    "        A_c = self.cache['A']\n",
    "        self.cache['DZ'] = np.multiply(self.Dactivation(A_c),DA_c)\n",
    "        \n",
    "    def backwardProp(self):\n",
    "        # get stored activation values\n",
    "        DZ_c = self.cache['DZ']\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        # calculate weight gradients\n",
    "        DWeight = np.dot(A_p.T,DZ_c) / A_p.shape[0]\n",
    "        Dbias = np.mean(DZ_c,axis=0)\n",
    "        Dcache = {'DWeight':DWeight,'Dbias':Dbias}\n",
    "        # calculate DZ_p, i.e. DZ of previous layer in network\n",
    "        DA_p = np.dot(DZ_c,self.Weight.T)\n",
    "        self.previousLayer.getDZ_c(DA_p)\n",
    "        \n",
    "        return Dcache\n",
    "        \n",
    "    def updateLayerParams(self,learningRate,Dcache):\n",
    "        DWeight, Dbias = Dcache['DWeight'], Dcache['Dbias']\n",
    "        self.Weight -= learningRate * DWeight\n",
    "        self.bias -= learningRate * Dbias\n",
    "        \n",
    "    def makeReady(self,previousLayer=None,nextLayer=None):\n",
    "        self.previousLayer = previousLayer\n",
    "        self.sizeIn = self.previousLayer.sizeOut\n",
    "        \n",
    "        self.nextLayer = nextLayer\n",
    "            \n",
    "        self.initializeWeightBias()\n",
    "        \n",
    "    def initializeWeightBias(self):\n",
    "        n_p,n_c = self.sizeIn[0],self.sizeOut[0]\n",
    "        self.Weight = np.random.randn(n_p,n_c) * 1 / (n_p + n_c)\n",
    "        self.bias = np.zeros((1,n_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution layer utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPictureDims(height_pl,width_pl,padding_cl,kernelParams_cl):\n",
    "    '''Calculates a convolutional layers height and width dimensions based on:\n",
    "    - previous (convolutional) layer shape\n",
    "    - type of padding used\n",
    "    - kernel size of curent layer'''\n",
    "    \n",
    "    stride = kernelParams_cl['stride']\n",
    "    if 'height_k' in kernelParams_cl: # dealing with a convolutional layer's request\n",
    "        height_k, width_k = kernelParams_cl['height_k'],kernelParams_cl['width_k']\n",
    "    elif 'height_pool' in kernelParams_cl: # dealing with a pooling layer's request\n",
    "        height_k, width_k = kernelParams_cl['height_pool'],kernelParams_cl['width_pool']\n",
    "    \n",
    "    if padding_cl == 'valid':\n",
    "        height_pad, width_pad = (0,0)\n",
    "    if padding_cl == 'same':\n",
    "        height_pad = np.ceil((stride*(height_pl-1)+height_k-height_pl) / 2)\n",
    "        width_pad = np.ceil((stride*(width_pl-1)+width_k-width_pl) / 2)\n",
    "        \n",
    "    height_cl = int(np.ceil((height_pl-height_k+1+2*height_pad) / stride))\n",
    "    width_cl = int(np.ceil((width_pl-width_k+1+2*width_pad) / stride))\n",
    "    \n",
    "    return height_cl, width_cl\n",
    "\n",
    "def getConvSliceCorners(h,w,height_k,width_k,stride):\n",
    "    '''Calculates and returns the edge indices of the slice in layer_p used to compute layer_c[h,w]'''\n",
    "    hStart, hEnd = (h * stride, h * stride + height_k)\n",
    "    wStart, wEnd = (w * stride, w * stride + width_k)\n",
    "    \n",
    "    return hStart,hEnd,wStart,wEnd\n",
    "\n",
    "def pad(Z,pad):\n",
    "    # Takes a four-dimensionan tensor tensor of shape (x1,x2,x3,x4,x5)\n",
    "    # Adds zero padding for dimensions x2 and x3 to create an array\n",
    "    # Zpadded of shape (x1,x2+2*pad,x3*pad,x4)\n",
    "    Zpadded = np.pad(Z,mode='constant',\n",
    "                     pad_width=((0,0),(0,0),(pad,pad),(pad,pad),(0,0)),\n",
    "                    constant_values=((0,0),(0,0),(0,0),(0,0),(0,0)))\n",
    "    \n",
    "    return Zpadded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvLayer(object):\n",
    "    '''Object class representing a convolutional layer in a feed-forward neural net'''\n",
    "    \n",
    "    def __init__(self,kernelHeight,kernelWidth,channels,\n",
    "                 stride,padding='valid',\n",
    "                 activation='tanh'):\n",
    "    \n",
    "        assert padding in ['same','valid']\n",
    "        assert activation in ['tanh','sigmoid','relu','identity']\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            self.activation = (tanh,activation)\n",
    "            self.Dactivation = Dtanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = (sigmoid,activation)\n",
    "            self.Dactivation = Dsigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activation = (relu,activation)\n",
    "            self.Dactivation = Drelu\n",
    "        elif activation == 'identity':\n",
    "            self.activation = (identity,activation)\n",
    "            self.Dactivation = Didentity\n",
    "            \n",
    "        self.padding = padding\n",
    "        self.sizeIn = None\n",
    "        self.sizeOut = [channels]\n",
    "        self.kernelParams = {'stride':stride,'height_k':kernelHeight,'width_k':kernelWidth}\n",
    "        \n",
    "        self.Weight = None\n",
    "        self.bias = None\n",
    "        self.cache = {'A':None,'DZ':None}\n",
    "        self.previousLayer = None\n",
    "        self.nextLayer = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns a string with bio of layer.'''\n",
    "        \n",
    "        bio = '----------------------------------------' \\\n",
    "                + '\\n Layer type: Convolution layer' \\\n",
    "                + '\\n Shape of kernel (width,height): ' + ','.join([str(self.kernelParams['height_k']),\n",
    "                                                                  str(self.kernelParams['width_k'])]) \\\n",
    "                + '\\n Stride used for kernel: ' + str(self.kernelParams['stride']) \\\n",
    "                + '\\n Shape of input data (channels,height,width): ' + str(self.sizeIn) \\\n",
    "                + '\\n Padding used: ' + self.padding \\\n",
    "                + '\\n Type of activation used: ' + self.activation[1] \\\n",
    "                + '\\n Shape of output data (channels,height,width): ' + str(self.sizeOut)\n",
    "            \n",
    "        return bio\n",
    "\n",
    "        \n",
    "    def forwardProp(self):\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        batchSize = A_p.shape[0]\n",
    "        \n",
    "        channels_c = self.sizeOut[0]\n",
    "        height_c = self.sizeOut[1]\n",
    "        width_c = self.sizeOut[2]\n",
    "        \n",
    "        height_k = self.kernelParams['height_k']\n",
    "        width_k = self.kernelParams['width_k']\n",
    "        stride = self.kernelParams['stride']\n",
    "        \n",
    "        Z_c = np.zeros((batchSize,channels_c,height_c,width_c,1))\n",
    "        \n",
    "        for h in range(height_c):\n",
    "            for w in range(width_c):\n",
    "                hStart,hEnd,wStart,wEnd = getConvSliceCorners(h,w,height_k,width_k,stride)\n",
    "                X_hw = np.multiply(A_p[:,:,hStart:hEnd,wStart:wEnd,:],self.Weight)\n",
    "                Y_hw = np.sum(X_hw,axis=(1,2,3))\n",
    "                Z_c[:,:,h,w,0] = Y_hw\n",
    "        \n",
    "        Z_c += self.bias\n",
    "        A_c = self.activation[0](Z_c)\n",
    "        self.cache['A'] = A_c\n",
    "        \n",
    "    def getDZ_c(self,DA_c):\n",
    "        # calculates this layer's DZ. gets called from next layer in network during backprop\n",
    "        A_c = self.cache['A']\n",
    "        self.cache['DZ'] = self.Dactivation(A_c) * DA_c\n",
    "        \n",
    "    def backwardProp(self):\n",
    "        # get stored activation values\n",
    "        DZ_c = self.cache['DZ']\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        batchSize = A_p.shape[0]\n",
    "        \n",
    "        channels_c = self.sizeOut[0]\n",
    "        height_c = self.sizeOut[1]\n",
    "        width_c = self.sizeOut[2]\n",
    "        \n",
    "        height_k = self.kernelParams['height_k']\n",
    "        width_k = self.kernelParams['width_k']\n",
    "        stride = self.kernelParams['stride']\n",
    "        \n",
    "        channels_p = self.sizeIn[0]\n",
    "        height_p = self.sizeIn[1]\n",
    "        width_p = self.sizeIn[2]\n",
    "        \n",
    "        # calculate weight gradients & DZ_p, i.e. DZ of previous layer in network\n",
    "        DWeight = np.zeros((1,channels_p,height_k,width_k,channels_c))\n",
    "        DZ_cback = np.transpose(DZ_c,(0,4,2,3,1))\n",
    "        \n",
    "        DA_p = np.zeros((batchSize,height_p,width_p,channels_p,1))\n",
    "        Weight_back = np.transpose(self.Weight,(0,4,2,3,1))\n",
    "        \n",
    "        for h in range(height_c):\n",
    "            for w in range(width_c):\n",
    "                hStart,hEnd,wStart,wEnd = getConvSliceCorners(h,w,height_k,width_k,stride)\n",
    "                #print('A')\n",
    "                I_hw = np.multiply(DZ_cback[:,:,h,w,:][:,:,np.newaxis,np.newaxis,:],\n",
    "                                   A_p[:,:,hStart:hEnd,wStart:wEnd,:])\n",
    "\n",
    "                J_hw = np.mean(I_hw,axis=0)\n",
    "\n",
    "                DWeight[0,:,:,:,:] += J_hw\n",
    "                \n",
    "                X_hw = np.multiply(DZ_c[:,:,h,w,:][:,:,np.newaxis,np.newaxis,:],\n",
    "                                   Weight_back)\n",
    "                #print('E')\n",
    "                Y_hw = np.sum(X_hw,axis=1)\n",
    "\n",
    "                DA_p[:,hStart:hEnd,wStart:wEnd,:,0] += Y_hw\n",
    "                \n",
    "        #DWeight /= DA_p.shape[0]\n",
    "        \n",
    "        Dbias = np.mean(np.sum(DZ_c,axis=(2,3,4)),axis=0)\n",
    "        Dbias = Dbias[np.newaxis,:,np.newaxis,np.newaxis,np.newaxis]\n",
    "        \n",
    "        Dcache = {'DWeight':DWeight,'Dbias':Dbias}\n",
    "        \n",
    "        DA_p = np.transpose(DA_p,(0,3,1,2,4))\n",
    "        self.previousLayer.getDZ_c(DA_p)\n",
    "        \n",
    "        return Dcache\n",
    "    \n",
    "    def updateLayerParams(self,learningRate,Dcache):\n",
    "        DWeight, Dbias = Dcache['DWeight'], Dcache['Dbias']\n",
    "        self.Weight -= learningRate * DWeight\n",
    "        self.bias -= learningRate * Dbias\n",
    "    \n",
    "    def makeReady(self,previousLayer=None,nextLayer=None):\n",
    "        self.previousLayer = previousLayer\n",
    "        self.sizeIn = self.previousLayer.sizeOut\n",
    "        height_pl, width_pl = self.sizeIn[1],self.sizeIn[2]\n",
    "        height_cl, width_cl = getPictureDims(height_pl,\n",
    "                                           width_pl,\n",
    "                                           self.padding,\n",
    "                                           self.kernelParams)\n",
    "        \n",
    "        self.sizeOut.extend([height_cl,width_cl])\n",
    "        \n",
    "        self.nextLayer = nextLayer\n",
    "            \n",
    "        self.initializeWeightBias()\n",
    "        \n",
    "    def initializeWeightBias(self):\n",
    "        self.Weight = np.random.randn(1,\n",
    "                                      self.sizeIn[0], # channels_pl\n",
    "                                      self.kernelParams['height_k'], # kernel height\n",
    "                                      self.kernelParams['width_k'], # kernel width\n",
    "                                      self.sizeOut[0]) # channels_cl\n",
    "        self.bias = np.zeros((1,self.sizeOut[0],1,1,1)) # channels_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PoolingLayer(object):\n",
    "    '''Pooling layer (either \"max\" or \"mean\") between convolutional/appropriate reshaping layers'''\n",
    "    \n",
    "    def __init__(self,poolingHeight,poolingWidth,\n",
    "                 stride,padding='valid',\n",
    "                 poolingType='max'):\n",
    "        \n",
    "        assert padding in ['same','valid']\n",
    "        assert poolingType in ['max','mean']\n",
    "        \n",
    "        self.padding = padding\n",
    "        self.poolingType = poolingType\n",
    "        self.sizeIn = None\n",
    "        self.sizeOut = None\n",
    "        self.poolingParams = {'stride':stride,'height_pool':poolingHeight,'width_pool':poolingWidth}\n",
    "        \n",
    "        self.cache = {'A':None,'DZ':None}\n",
    "        self.previousLayer = None\n",
    "        self.nextLayer = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns a string with bio of layer.'''\n",
    "        \n",
    "        bio = '----------------------------------------' \\\n",
    "                + '\\n Layer type: Pooling layer (' + str(self.poolingType) +')' \\\n",
    "                + '\\n Shape of pool (width,height): ' + ','.join([str(self.poolingParams['height_pool']),\n",
    "                                                                  str(self.poolingParams['width_pool'])]) \\\n",
    "                + '\\n Stride used for pool: ' + str(self.poolingParams['stride']) \\\n",
    "                + '\\n Shape of input data (channels,height,width): ' + str(self.sizeIn) \\\n",
    "                + '\\n Padding used: ' + self.padding \\\n",
    "                + '\\n Shape of output data (channels,height,width): ' + str(self.sizeOut)\n",
    "            \n",
    "        return bio\n",
    "        \n",
    "    def forwardProp(self):\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        batchSize = A_p.shape[0]\n",
    "        \n",
    "        channels_c = self.sizeOut[0]\n",
    "        height_c = self.sizeOut[1]\n",
    "        width_c = self.sizeOut[2]\n",
    "        \n",
    "        height_pool = self.poolingParams['height_pool']\n",
    "        width_pool = self.poolingParams['width_pool']\n",
    "        stride = self.poolingParams['stride']\n",
    "        \n",
    "        Z_c = np.zeros((batchSize,channels_c,height_c,width_c,1))\n",
    "                \n",
    "        for h in range(height_c):\n",
    "            for w in range(width_c):\n",
    "                hStart,hEnd,wStart,wEnd = getConvSliceCorners(h,w,height_pool,width_pool,stride)\n",
    "                X_hw = A_p[:,:,hStart:hEnd,wStart:wEnd,0]\n",
    "                \n",
    "                if self.poolingType == 'max':\n",
    "                    Y_hw = np.amax(X_hw,axis=(2,3))\n",
    "                elif self.poolingType == 'mean':\n",
    "                    Y_hw = np.mean(X_hw,axis=(2,3))\n",
    "                    \n",
    "                Z_c[:,:,h,w,0] = Y_hw\n",
    "        \n",
    "        self.cache['A'] = Z_c # pooling layer has no activation, hence A_c = Z_c\n",
    "        \n",
    "    def getDZ_c(self,DA_c):\n",
    "        # calculates this layer's DZ. gets called from next layer in network during backprop\n",
    "        self.cache['DZ'] = DA_c # since for pooling layers A = Z -> DA = DZ\n",
    "        \n",
    "    def backwardProp(self):\n",
    "        # get stored activation values\n",
    "        DZ_c = self.cache['DZ']\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        batchSize = A_p.shape[0]\n",
    "        \n",
    "        channels_c = self.sizeOut[0]\n",
    "        height_c = self.sizeOut[1]\n",
    "        width_c = self.sizeOut[2]\n",
    "        \n",
    "        height_pool = self.poolingParams['height_pool']\n",
    "        width_pool = self.poolingParams['width_pool']\n",
    "        stride = self.poolingParams['stride']\n",
    "        \n",
    "        channels_p = self.sizeIn[0]\n",
    "        height_p = self.sizeIn[1]\n",
    "        width_p = self.sizeIn[2]\n",
    "\n",
    "        # calculate weight gradients & DZ_p, i.e. DZ of previous layer in network\n",
    "        \n",
    "        DA_p = np.zeros((batchSize,channels_p,height_p,width_p,1))\n",
    "        \n",
    "        for h in range(height_c):\n",
    "            for w in range(width_c):\n",
    "                hStart,hEnd,wStart,wEnd = getConvSliceCorners(h,w,height_pool,width_pool,stride)\n",
    "                \n",
    "                if self.poolingType == 'max':\n",
    "                    X_hw = A_p[:,:,hStart:hEnd,wStart:wEnd,0]\n",
    "                    #print(\"From within pooling layer's backprop:\")\n",
    "                    #print(\"Shape of X_hw:\",X_hw.shape)\n",
    "                    Y_hw = np.amax(X_hw,(2,3))[:,:,np.newaxis,np.newaxis]\n",
    "                    #print(\"Shape of Y_hw:\",Y_hw.shape)\n",
    "                    U_hw = DZ_c[:,:,h,w,:][:,:,np.newaxis,np.newaxis,:]\n",
    "                    #print('Shape of U_hw:',U_hw.shape)\n",
    "                    V_hw = (X_hw==Y_hw)[:,:,:,:,np.newaxis]\n",
    "                    #print('Shape of V_hw:',V_hw.shape)\n",
    "                    #print('Shape of updated DA_p slice:',DA_p[:,:,hStart:hEnd,wStart:wEnd,:].shape)\n",
    "                    DA_p[:,:,hStart:hEnd,wStart:wEnd,:] += U_hw * V_hw\n",
    "                elif self.poolingType == 'mean':\n",
    "                    X_hw = 1/(height_pool * width_pool) * np.ones(batchSize,channels_p,height_pool,width_pool,1)\n",
    "                    DA_p[:,:,hStart:hEnd,wStart:wEnd,:] += np.multiply(DZ_c[:,:,h,w,:][:,:,np.newaxis,np.newaxis,:],\n",
    "                                                                       X_hw)\n",
    "        \n",
    "        self.previousLayer.getDZ_c(DA_p)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def updateLayerParams(self,learningRate,Dcache):\n",
    "        \"\"\"Bogus class to make neural network's backprop more homogenuous\"\"\"\n",
    "        return\n",
    "    \n",
    "    def makeReady(self,previousLayer=None,nextLayer=None):\n",
    "        self.previousLayer = previousLayer\n",
    "        self.sizeIn = self.previousLayer.sizeOut\n",
    "        \n",
    "        self.sizeOut = [self.sizeIn[0]] # number of channels remains unchanged through pooling\n",
    "        height_pl, width_pl = self.sizeIn[1],self.sizeIn[2]\n",
    "        height_cl, width_cl = getPictureDims(height_pl,\n",
    "                                           width_pl,\n",
    "                                           self.padding,\n",
    "                                           self.poolingParams)\n",
    "        \n",
    "        self.sizeOut.extend([height_cl,width_cl])\n",
    "        \n",
    "        self.nextLayer = nextLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected to convolution reshaping layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FcToConv(object):\n",
    "    '''Transitional layer handling reshaping between fclayer activations -> convLayer activations,\n",
    "    and convLayer activation derivatives -> fcLayer activation derivatives.'''\n",
    "    \n",
    "    def __init__(self,convDims):\n",
    "\n",
    "        self.cache = {'A':None,'DZ':None}\n",
    "        self.previousLayer = None\n",
    "        self.nextLayer = None\n",
    "        \n",
    "        self.sizeIn = None\n",
    "        \n",
    "        [convChannels,convHeight,convWidth] = convDims\n",
    "        self.sizeOut = [convChannels,convHeight,convWidth]\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns a string with bio of layer.'''\n",
    "        bio = '----------------------------------------' \\\n",
    "                + '\\n Layer type: Reshaping layer (Fully connected -> Convolution)' \\\n",
    "                + '\\n Shape of input data: ' + str(self.sizeIn) \\\n",
    "                + '\\n Shape of output data: ' + str(self.sizeOut)\n",
    "            \n",
    "        return bio\n",
    "        \n",
    "    def forwardProp(self):\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        batchSize = A_p.shape[0]\n",
    "        aShape = [batchSize,self.sizeOut[0],sizeOut[1],sizeOut[2],1]\n",
    "        A_c = Z_c = A_p.reshape(aShape)\n",
    "        #print(\"From within reshape (conv -> fc) layer's forwardProp:\")\n",
    "        #print(\"Shape of previous layer's activation:\",A_p.shape)\n",
    "        #print(\"Shape of current layer's activation:\",A_c.shape)\n",
    "        #print(\"------------------------------------\")\n",
    "        self.cache['A'] = A_c\n",
    "        \n",
    "    def getDZ_c(self,DA_c):\n",
    "        # calculates this layer's DZ. gets called from next layer in network during backprop\n",
    "        batchSize = DA_c.shape[0]\n",
    "        dzShape = [batchSize, self.sizeIn[0]]\n",
    "        self.cache['DZ'] = DA_c.reshape(dzShape)\n",
    "        \n",
    "    def backwardProp(self):\n",
    "        # get stored activation values\n",
    "        DZ_c = self.cache['DZ']\n",
    "        # calculate DZ_p, i.e. DZ of previous layer in network\n",
    "        DA_p = DZ_c\n",
    "        self.previousLayer.getDZ_c(DA_p)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def updateLayerParams(self,learningRate,Dcache):\n",
    "        # bogus function for layer consistency from the neural net class point of view\n",
    "        return\n",
    "    \n",
    "    def makeReady(self,previousLayer=None,nextLayer=None):\n",
    "        self.previousLayer = previousLayer\n",
    "        self.sizeIn = self.previousLayer.sizeOut\n",
    "        \n",
    "        self.nextLayer = nextLayer\n",
    "        self.convShape = self.nextLayer.convShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution to fully connected reshaping layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvToFC(object):\n",
    "    '''Transitional layer handling reshaping between convlayer activations -> fcLayer activations,\n",
    "    and fcLayer activation derivatives -> convLayer activation derivatives.'''\n",
    "    \n",
    "    def __init__(self,n):\n",
    "\n",
    "        self.cache = {'A':None,'DZ':None}\n",
    "        self.previousLayer = None\n",
    "        self.nextLayer = None\n",
    "        \n",
    "        self.sizeIn = None\n",
    "        self.sizeOut = [n]\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns a string with bio of layer.'''\n",
    "        bio = '----------------------------------------' \\\n",
    "                + '\\n Layer type: Reshaping layer (Convolution -> Fully connected)' \\\n",
    "                + '\\n Shape of input data: ' + str(self.sizeIn) \\\n",
    "                + '\\n Shape of output data: ' + str(self.sizeOut)\n",
    "            \n",
    "        return bio\n",
    "        \n",
    "    def forwardProp(self):\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        batchSize = A_p.shape[0]\n",
    "        #A_c = A_p.reshape([batchSize].extend(self.sizeOut))\n",
    "        A_c = A_p.reshape([batchSize,self.sizeOut[0]])\n",
    "        #print(\"From within reshape (conv -> fc) layer's forwardProp:\")\n",
    "        #print(\"Shape of previous layer's activation:\",A_p.shape)\n",
    "        #print(\"Shape of current layer's activation:\",A_c.shape)\n",
    "        #print(\"------------------------------------\")\n",
    "        self.cache['A'] = A_c\n",
    "        \n",
    "    def getDZ_c(self,DA_c):\n",
    "        # calculates this layer's DZ. gets called from next layer in network during backprop\n",
    "        batchSize = DA_c.shape[0]\n",
    "        #dzShape = [batchSize].extend(self.sizeIn).extend(1)\n",
    "        dzShape = [batchSize,self.sizeIn[0],self.sizeIn[1],self.sizeIn[2],1]\n",
    "        self.cache['DZ'] = DA_c.reshape(dzShape)\n",
    "        \n",
    "    def backwardProp(self):\n",
    "        # get stored activation values\n",
    "        DZ_c = self.cache['DZ']\n",
    "        # calculate DZ_p, i.e. DZ of previous layer in network\n",
    "        DA_p = DZ_c\n",
    "        self.previousLayer.getDZ_c(DA_p)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def updateLayerParams(self,learningRate,Dcache):\n",
    "        # bogus function for layer consistency from the neural net class point of view\n",
    "        return\n",
    "    \n",
    "    def makeReady(self,previousLayer=None,nextLayer=None):\n",
    "        self.previousLayer = previousLayer\n",
    "        self.sizeIn = self.previousLayer.sizeOut\n",
    "        \n",
    "        self.nextLayer = nextLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout(object):\n",
    "    '''Dropout layer acting between \"real\" network layers.'''\n",
    "    \n",
    "    def __init__(self,dropoutRate):\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.cache = {'A':None,'DZ':None,'outDropper':None}\n",
    "        self.previousLayer = None\n",
    "        self.nextLayer = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns a string with bio of layer.'''\n",
    "        bio = '----------------------------------------' \\\n",
    "                + '\\n Layer type: Dropout layer' \\\n",
    "                + '\\n Dropout rate: ' + str(self.dropoutRate) \\\n",
    "                + '\\n Shape of input/output data: ' + str(self.sizeOut)\n",
    "            \n",
    "        return bio\n",
    "        \n",
    "        \n",
    "    def forwardProp(self):\n",
    "        A_p = self.previousLayer.cache['A']\n",
    "        outDropper = np.random.choice([0,1],size=A_p.shape,p=[self.dropoutRate,1-self.dropoutRate])\n",
    "        A_c = Z_c = np.multiply(outDropper,A_p)\n",
    "        self.cache['A'] = A_c\n",
    "        self.cache['outDropper'] = outDropper\n",
    "        \n",
    "    def getDZ_c(self,DA_c):\n",
    "        DZ_c = DA_c\n",
    "        self.cache['DZ'] = DZ_c\n",
    "        \n",
    "    def backwardProp(self):\n",
    "        DZ_c = self.cache['DZ']\n",
    "        outDropper = self.cache['outDropper']\n",
    "        DA_p = np.multiply(DZ_c,outDropper)\n",
    "        self.previousLayer.getDZ_c(DA_p)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def updateLayerParams(self,learningRate,Dcache):\n",
    "        # bogus function for layer consistency from the neural net class point of view\n",
    "        return\n",
    "        \n",
    "    def makeReady(self,previousLayer=None,nextLayer=None):\n",
    "        self.previousLayer = previousLayer\n",
    "        self.sizeIn = self.sizeOut = self.previousLayer.sizeOut\n",
    "        \n",
    "        self.nextLayer = nextLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secret input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _InputLayer(object):\n",
    "    '''Input layer created automatically by network once the training data shape/kind is known to it.'''\n",
    "    \n",
    "    def __init__(self,pad=0):\n",
    "        self.sizeOut = None\n",
    "        self.cache = {'A':None}\n",
    "        self.nextLayer = None\n",
    "        self.flatData = None\n",
    "        self.pad = pad\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns a string with bio of layer.'''\n",
    "        \n",
    "        if self.flatData:\n",
    "            secondLine = '\\n Number of neurons in input/output data: ' + str(self.sizeOut)\n",
    "        elif not self.flatData:\n",
    "            secondLine = '\\n Shape of input/output data (channels,height,width): ' + str(self.sizeOut)\n",
    "        \n",
    "        bio = '----------------------------------------' \\\n",
    "                + '\\n Layer type: (Secret) input layer' \\\n",
    "                + secondLine\n",
    "            \n",
    "        return bio\n",
    "        \n",
    "    def forwardProp(self,XBatch):\n",
    "        if self.flatData:\n",
    "            self.cache['A'] = XBatch # -> input data is 2 dimensional, (bachSize,nFeature)\n",
    "            #print(\"From within secret input layer's forward prop:\")\n",
    "            #print(\"Type of input batch without adding bogus dimension:\",type(self.cache['A']))\n",
    "        elif not self.flatData:\n",
    "            self.cache['A'] = pad(np.expand_dims(XBatch,-1),self.pad) # -> input data is 4 dim.,(sampleSize,channels,height,width)\n",
    "            #print(\"From within secret input layer's forward prop:\")\n",
    "            #print(\"Shape of input batch after adding bogus dimension:\",self.cache['A'].shape)\n",
    "            \n",
    "        #print(\"-------------------------------\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def getDZ_c(self,DA_c):\n",
    "        # bogus function for layer consistency from the neural net class point of view\n",
    "        return\n",
    "    \n",
    "    def makeReady(self,nextLayer=None,XSample=None):\n",
    "        self.nextLayer = nextLayer\n",
    "        self.sizeOut = self.getSizeOutFromX(XSample)\n",
    "        \n",
    "    def getSizeOutFromX(self,XSample):\n",
    "        if len(XSample.shape) == 2: # -> assume X is flattened array of shape (sampleSize,nFeature)\n",
    "            sizeOut = [XSample.shape[1]]\n",
    "            \n",
    "            self.flatData = True\n",
    "            \n",
    "            return sizeOut\n",
    "        \n",
    "        elif len(XSample.shape) == 4: # -> assume X is high dim. tensor of shape (sampleSize,channels,height,width)\n",
    "            inputChannels,inputHeight,inputWidth = XSample.shape[1:]\n",
    "            sizeOut = [inputChannels,inputHeight+2*self.pad,inputWidth+2*self.pad]\n",
    "            \n",
    "            self.flatData = False\n",
    "            \n",
    "            return sizeOut\n",
    "        \n",
    "        else:\n",
    "            print('''X has to be either of shape [nSamples,nFeatures] or, for images,\n",
    "            [imageChannels,imageHeight,imageWidth]. Please reshape your training data and\n",
    "            try compiling the model again.''')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFNetwork(object):\n",
    "    \n",
    "    def __init__(self,initPad=0):\n",
    "        self.initPad = initPad\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self._inputLayer = None\n",
    "        self.dataType = None # will indicate wether flattened feature vectors or high dim image tensors\n",
    "        self.finalState = False\n",
    "        self.trained = False\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Print out structure of neural net (if it has been fixated).'''\n",
    "        \n",
    "        if self.finalState:\n",
    "            bluePrint = '\\n'.join([self._inputLayer.__str__()] + [layer.__str__() for layer in self.layers])\n",
    "            \n",
    "            return bluePrint\n",
    "        else:\n",
    "            print('The model has to be fixated first.')\n",
    "        \n",
    "    def addFCLayer(self,n,activation='tanh'):\n",
    "        '''Adds a fully connected layer to the neural network.'''\n",
    "        \n",
    "        fullyConnectedLayer = FcLayer(n,activation)\n",
    "        \n",
    "        self.layers.append(fullyConnectedLayer)\n",
    "        \n",
    "    def addConvLayer(self,kernelHeight,kernelWidth,channels,stride,padding='valid',activation='tanh'):\n",
    "        '''Adds a convolution layer to the neural network.'''\n",
    "        \n",
    "        convolutionLayer = ConvLayer(kernelHeight,\n",
    "                                     kernelWidth,\n",
    "                                     channels,\n",
    "                                     stride,\n",
    "                                     padding,activation)\n",
    "        \n",
    "        self.layers.append(convolutionLayer)\n",
    "        \n",
    "    def addPoolingLayer(self,poolingHeight,poolingWidth,stride,padding='valid',poolingType='max'):\n",
    "        '''Adds a pooling layer to the neural network. Recommended after convolutional layers.'''\n",
    "        \n",
    "        poolingLayer = PoolingLayer(poolingHeight,\n",
    "                                    poolingWidth,\n",
    "                                    stride,\n",
    "                                    padding,\n",
    "                                    poolingType)\n",
    "        \n",
    "        self.layers.append(poolingLayer)\n",
    "\n",
    "        \n",
    "    def addFCToConvReshapeLayer(self,convDims):\n",
    "        '''Adds a reshaping layer to the neural network. Necessary to link up a fully connected layer\n",
    "        with a subsequent convolution layer.'''\n",
    "        \n",
    "        shapeFullyConnectedToConvolution = FcToConv(convDims)\n",
    "        \n",
    "        self.layers.append(shapeFullyConnectedToConvolution)\n",
    "        \n",
    "    def addConvToFCReshapeLayer(self,n):\n",
    "        '''Adds a reshaping layer to the neural network. Necessary to link up a convolutional layer with a \n",
    "        subsequent fully connected layer.'''\n",
    "        \n",
    "        shapeConvolutionalToFullyConnected = ConvToFC(n)\n",
    "        \n",
    "        self.layers.append(shapeConvolutionalToFullyConnected)\n",
    "        \n",
    "    def addDropoutLayer(self,dropoutRate):\n",
    "        '''Adds a dropout layer.'''\n",
    "        \n",
    "        dropoutLayer = Dropout(dropoutRate)\n",
    "        \n",
    "        self.layers.append(dropoutLayer)\n",
    "        \n",
    "    def fixateNetwork(self,XSample):\n",
    "        '''Fixes model, finalising its blue-print.\n",
    "        Attaches loss function to model.\n",
    "        Creates hidden input layer based on shape of passed sample.\n",
    "        Calls each layer's makeReady() method.'''\n",
    "        \n",
    "        # only do stuff if model hasnt allready been fixated\n",
    "        if self.finalState:\n",
    "            print('This model has already been fixated.')\n",
    "            \n",
    "            return\n",
    "        \n",
    "        # add secret input layer and make ready\n",
    "        self._inputLayer = _InputLayer(self.initPad)\n",
    "        self._inputLayer.makeReady(self.layers[0],XSample)\n",
    "        \n",
    "        # iterate through layers and introduce to immediate neighouring layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0: # first layer, use _inputLayer as previous layer\n",
    "                previousLayer = self._inputLayer\n",
    "            else: # at least second layer, so pass previous layer\n",
    "                previousLayer = self.layers[i-1]\n",
    "            \n",
    "            if i == len(self.layers) - 1: # last user made layer in network, no next layer exists\n",
    "                nextLayer = None\n",
    "            else: # at most second to last layer, pass next layer\n",
    "                nextLayer = self.layers[i+1]\n",
    "                \n",
    "            layer.makeReady(previousLayer,nextLayer)\n",
    "        \n",
    "        lastLayer = self.layers[-1]\n",
    "        \n",
    "        # attach loss function to neural net depending on last fully connected layer's activation type\n",
    "        if lastLayer.activation[0] == sigmoid:\n",
    "            self.loss = sigmoidLoss\n",
    "        elif lastLayer.activation[0] == softmax:\n",
    "            self.loss = softmaxLoss\n",
    "        else:\n",
    "            print('The last layer needs to have either \"softmax\" or \"sigmoid\" activation. Model was not fixated')\n",
    "        \n",
    "        self.finalState = True\n",
    "        \n",
    "    def trainNetwork(self,nEpochs,learningRate,batchSize,X,y,displaySteps=50,oneHotY = True):\n",
    "        '''Trains the neural network using naive gradient descent.'''\n",
    "        # vectorize Y to one-hot format if needed (default is True)\n",
    "        if oneHotY:\n",
    "            Y = self.oneHotY(y)\n",
    "        elif not oneHotY:\n",
    "            Y = y\n",
    "        \n",
    "        # initialize storage for batch losses to be collected during training\n",
    "        lossHistory = []\n",
    "        recentLoss = 0\n",
    "        \n",
    "        # execute training\n",
    "        for epoch in range(nEpochs):\n",
    "            for i,(XBatch,YBatch) in enumerate(self.getBatches(X,Y,batchSize)):\n",
    "                P = self.forwardProp(XBatch)\n",
    "                batchLoss = self.loss(P,YBatch)\n",
    "                recentLoss += batchLoss\n",
    "                self.backwardProp(learningRate,YBatch)\n",
    "                \n",
    "                if (i % displayStep) == 0 and (i != 0):\n",
    "                    averageRecentLoss = recentLoss / displaySteps\n",
    "                    lossHistory.append(averageRecentLoss)\n",
    "                    recentLoss = 0\n",
    "                    print('Epoch', epoch)\n",
    "                    print('Batch', i)\n",
    "                    print('Loss averaged over last '+str(displaySteps)+' batches',averageRecentLoss)\n",
    "        \n",
    "        # announce end of training\n",
    "        self.trained = True\n",
    "        print('---------------------------------------------------')\n",
    "        print('Training finished.')\n",
    "        print('nEpochs:',nEpochs)\n",
    "        print('learningRate:',learningRate)\n",
    "        print('batchSize:',batchSize)\n",
    "        \n",
    "        return lossHistory\n",
    "        \n",
    "    def oneHotY(self,y):\n",
    "        '''One hot vectorizes a target class index list into a [nData,nClasses] array.'''\n",
    "        \n",
    "        nClasses = len(np.unique(y.reshape(-1)))\n",
    "        Y = np.eye(nClasses)[y.reshape(-1)]\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def getBatches(self,X,Y,batchSize):\n",
    "        '''Sample randomly from X and Y, then yield batches.'''\n",
    "        nData = X.shape[0]\n",
    "        shuffledIndices = np.arange(nData)\n",
    "        np.random.shuffle(shuffledIndices)\n",
    "        \n",
    "        XShuffled, YShuffled = X[shuffledIndices], Y[shuffledIndices]\n",
    "        \n",
    "        nBatches = int(X.shape[0] / batchSize)\n",
    "        \n",
    "        for iBatch in range(nBatches):\n",
    "            XBatch, YBatch = (XShuffled[iBatch*batchSize:(iBatch+1)*batchSize],\n",
    "                              YShuffled[iBatch*batchSize:(iBatch+1)*batchSize])\n",
    "        \n",
    "            yield XBatch, YBatch\n",
    "        \n",
    "    def forwardProp(self,XBatch):\n",
    "        '''Executes one forward propagation through the network. Returns the loss averaged over the batch.'''\n",
    "        \n",
    "        self._inputLayer.forwardProp(XBatch) # feed training batch into network\n",
    "        \n",
    "        for layer in self.layers: # forward propagate through the network\n",
    "            layer.forwardProp()\n",
    "            \n",
    "        P = self.layers[-1].cache['A'] # get prediction\n",
    "            \n",
    "        return P\n",
    "        \n",
    "    def backwardProp(self,learningRate,YBatch):\n",
    "        '''Executes one backward propagation through the network. Returns the network's parameter's gradients'''\n",
    "        \n",
    "        P = self.layers[-1].cache['A']\n",
    "        self.layers[-1].cache['DZ'] = (P - YBatch) #/ YBatch.shape[0]\n",
    "        \n",
    "        for i,layer in enumerate(reversed(self.layers)):\n",
    "            layerDCache = layer.backwardProp()\n",
    "            layer.updateLayerParams(learningRate,layerDCache)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        '''If model is trained, performs forward prop and returns the prediction array.'''\n",
    "        \n",
    "        if not self.trained:\n",
    "            print('Model needs to be trained first.')\n",
    "            \n",
    "            return\n",
    "        \n",
    "        P = self.forwardProp(X)\n",
    "        \n",
    "        return np.argmax(P,axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply network class to mnist - build fullly connected net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      " Layer type: (Secret) input layer\n",
      " Number of neurons in input/output data: [784]\n",
      "----------------------------------------\n",
      " Layer type: Fully connected layer\n",
      " Number of neurons in input data: [784]\n",
      " Type of activation used: tanh\n",
      " Number of neurons in output data: [300]\n",
      "----------------------------------------\n",
      " Layer type: Dropout layer\n",
      " Dropout rate: 0.5\n",
      " Shape of input/output data: [300]\n",
      "----------------------------------------\n",
      " Layer type: Fully connected layer\n",
      " Number of neurons in input data: [300]\n",
      " Type of activation used: tanh\n",
      " Number of neurons in output data: [300]\n",
      "----------------------------------------\n",
      " Layer type: Dropout layer\n",
      " Dropout rate: 0.5\n",
      " Shape of input/output data: [300]\n",
      "----------------------------------------\n",
      " Layer type: Fully connected layer\n",
      " Number of neurons in input data: [300]\n",
      " Type of activation used: softmax\n",
      " Number of neurons in output data: [10]\n",
      "Epoch 0\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 2.34834674567\n",
      "Epoch 0\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 2.30153176704\n",
      "Epoch 0\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 2.21202317236\n",
      "Epoch 0\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 1.61422010801\n",
      "Epoch 0\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.87745421732\n",
      "Epoch 0\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.631105253877\n",
      "Epoch 0\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.47962951346\n",
      "Epoch 0\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.428201054477\n",
      "Epoch 0\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.431269058434\n",
      "Epoch 0\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.365908154031\n",
      "Epoch 0\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.366900664332\n",
      "Epoch 0\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.341100406854\n",
      "Epoch 0\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.342646964803\n",
      "Epoch 0\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.361746756071\n",
      "Epoch 0\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.336439475532\n",
      "Epoch 0\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.295508226374\n",
      "Epoch 0\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.297527107995\n",
      "Epoch 0\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.298308283291\n",
      "Epoch 0\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.289807021416\n",
      "Epoch 0\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.290032759263\n",
      "Epoch 0\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.293054946005\n",
      "Epoch 0\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.256412725543\n",
      "Epoch 0\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.290624225813\n",
      "Epoch 1\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.576578924285\n",
      "Epoch 1\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 0.246098240605\n",
      "Epoch 1\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 0.225168059211\n",
      "Epoch 1\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 0.24744451474\n",
      "Epoch 1\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.248300045049\n",
      "Epoch 1\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.242187356924\n",
      "Epoch 1\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.26452175983\n",
      "Epoch 1\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.277794648844\n",
      "Epoch 1\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.254946864733\n",
      "Epoch 1\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.261695492084\n",
      "Epoch 1\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.237973824134\n",
      "Epoch 1\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.216214620579\n",
      "Epoch 1\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.211509186044\n",
      "Epoch 1\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.217728909149\n",
      "Epoch 1\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.216205340607\n",
      "Epoch 1\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.20894954253\n",
      "Epoch 1\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.237360661488\n",
      "Epoch 1\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.22525877164\n",
      "Epoch 1\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.231213970235\n",
      "Epoch 1\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.215892365907\n",
      "Epoch 1\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.210054988853\n",
      "Epoch 1\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.238024602751\n",
      "Epoch 1\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.220125998886\n",
      "Epoch 2\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.396551925641\n",
      "Epoch 2\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 0.215996738923\n",
      "Epoch 2\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 0.203503952712\n",
      "Epoch 2\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 0.199353998259\n",
      "Epoch 2\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.229215735963\n",
      "Epoch 2\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.194600972402\n",
      "Epoch 2\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.186004424828\n",
      "Epoch 2\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.181802428726\n",
      "Epoch 2\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.203012158296\n",
      "Epoch 2\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.203689760722\n",
      "Epoch 2\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.211081229001\n",
      "Epoch 2\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.174773152536\n",
      "Epoch 2\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.195844915795\n",
      "Epoch 2\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.206596054399\n",
      "Epoch 2\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.203683087976\n",
      "Epoch 2\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.19446641783\n",
      "Epoch 2\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.200296306635\n",
      "Epoch 2\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.186033969483\n",
      "Epoch 2\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.190706916233\n",
      "Epoch 2\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.179843143382\n",
      "Epoch 2\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.207459383671\n",
      "Epoch 2\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.203414261677\n",
      "Epoch 2\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.194277502266\n",
      "Epoch 3\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.396621770711\n",
      "Epoch 3\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 0.168316233532\n",
      "Epoch 3\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 0.171147459584\n",
      "Epoch 3\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 0.194181159927\n",
      "Epoch 3\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.172091151472\n",
      "Epoch 3\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.16025201993\n",
      "Epoch 3\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.186525334241\n",
      "Epoch 3\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.173093673679\n",
      "Epoch 3\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.162310183532\n",
      "Epoch 3\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.19710891208\n",
      "Epoch 3\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.17467105097\n",
      "Epoch 3\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.16476637297\n",
      "Epoch 3\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.179596469159\n",
      "Epoch 3\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.160155753556\n",
      "Epoch 3\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.185669713746\n",
      "Epoch 3\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.157700459347\n",
      "Epoch 3\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.155496964991\n",
      "Epoch 3\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.193097364453\n",
      "Epoch 3\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.186971893287\n",
      "Epoch 3\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.186280869514\n",
      "Epoch 3\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.201064812706\n",
      "Epoch 3\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.167310529761\n",
      "Epoch 3\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.192760506618\n",
      "Epoch 4\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.348735730334\n",
      "Epoch 4\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 0.192929874483\n",
      "Epoch 4\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 0.169509101047\n",
      "Epoch 4\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 0.189765491537\n",
      "Epoch 4\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.143403714803\n",
      "Epoch 4\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.175937647517\n",
      "Epoch 4\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.166575058484\n",
      "Epoch 4\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.160716602852\n",
      "Epoch 4\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.146941878963\n",
      "Epoch 4\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.165678825774\n",
      "Epoch 4\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.164233404207\n",
      "Epoch 4\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.142210791524\n",
      "Epoch 4\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.161181570277\n",
      "Epoch 4\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.149929274492\n",
      "Epoch 4\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.177902558196\n",
      "Epoch 4\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.193033607193\n",
      "Epoch 4\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.17121489734\n",
      "Epoch 4\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.150541484262\n",
      "Epoch 4\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.16334114183\n",
      "Epoch 4\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.167666243352\n",
      "Epoch 4\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.165445587074\n",
      "Epoch 4\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.147266776433\n",
      "Epoch 4\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.148719466177\n",
      "Epoch 5\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.314340214561\n",
      "Epoch 5\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 0.134505039333\n",
      "Epoch 5\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 0.135969278865\n",
      "Epoch 5\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 0.159896561312\n",
      "Epoch 5\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.152721909231\n",
      "Epoch 5\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.167286028986\n",
      "Epoch 5\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.167199466864\n",
      "Epoch 5\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.126488050552\n",
      "Epoch 5\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.162834039289\n",
      "Epoch 5\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.139269355947\n",
      "Epoch 5\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.142457484855\n",
      "Epoch 5\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.15729750549\n",
      "Epoch 5\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.153123732178\n",
      "Epoch 5\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.156091935999\n",
      "Epoch 5\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.187866174625\n",
      "Epoch 5\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.188391080831\n",
      "Epoch 5\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.159178968863\n",
      "Epoch 5\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.165495332004\n",
      "Epoch 5\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.171036836602\n",
      "Epoch 5\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.15974200377\n",
      "Epoch 5\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.15648200885\n",
      "Epoch 5\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.146532480872\n",
      "Epoch 5\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.161515802397\n",
      "Epoch 6\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.292679547398\n",
      "Epoch 6\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 0.142358288662\n",
      "Epoch 6\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 0.143900787837\n",
      "Epoch 6\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 0.152283435433\n",
      "Epoch 6\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.141876763269\n",
      "Epoch 6\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.14559295695\n",
      "Epoch 6\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.151857437076\n",
      "Epoch 6\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.141391285101\n",
      "Epoch 6\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.138720972715\n",
      "Epoch 6\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.146597384858\n",
      "Epoch 6\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.124945543629\n",
      "Epoch 6\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.127658729798\n",
      "Epoch 6\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.154429169035\n",
      "Epoch 6\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.129055096228\n",
      "Epoch 6\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.125948170867\n",
      "Epoch 6\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.127065124034\n",
      "Epoch 6\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.15673954858\n",
      "Epoch 6\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.168001756592\n",
      "Epoch 6\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.149370820362\n",
      "Epoch 6\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.137941287428\n",
      "Epoch 6\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.171255605552\n",
      "Epoch 6\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.133160687836\n",
      "Epoch 6\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.143570387839\n",
      "Epoch 7\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.306281907585\n",
      "Epoch 7\n",
      "Batch 100\n",
      "Loss averaged over last 50 batches 0.151392788817\n",
      "Epoch 7\n",
      "Batch 150\n",
      "Loss averaged over last 50 batches 0.146497687779\n",
      "Epoch 7\n",
      "Batch 200\n",
      "Loss averaged over last 50 batches 0.138178866362\n",
      "Epoch 7\n",
      "Batch 250\n",
      "Loss averaged over last 50 batches 0.126978812823\n",
      "Epoch 7\n",
      "Batch 300\n",
      "Loss averaged over last 50 batches 0.164415847147\n",
      "Epoch 7\n",
      "Batch 350\n",
      "Loss averaged over last 50 batches 0.140156727939\n",
      "Epoch 7\n",
      "Batch 400\n",
      "Loss averaged over last 50 batches 0.133260827696\n",
      "Epoch 7\n",
      "Batch 450\n",
      "Loss averaged over last 50 batches 0.116945271172\n",
      "Epoch 7\n",
      "Batch 500\n",
      "Loss averaged over last 50 batches 0.127410471441\n",
      "Epoch 7\n",
      "Batch 550\n",
      "Loss averaged over last 50 batches 0.148589327978\n",
      "Epoch 7\n",
      "Batch 600\n",
      "Loss averaged over last 50 batches 0.138558679879\n",
      "Epoch 7\n",
      "Batch 650\n",
      "Loss averaged over last 50 batches 0.140834112862\n",
      "Epoch 7\n",
      "Batch 700\n",
      "Loss averaged over last 50 batches 0.16048563678\n",
      "Epoch 7\n",
      "Batch 750\n",
      "Loss averaged over last 50 batches 0.127874820948\n",
      "Epoch 7\n",
      "Batch 800\n",
      "Loss averaged over last 50 batches 0.152103321273\n",
      "Epoch 7\n",
      "Batch 850\n",
      "Loss averaged over last 50 batches 0.140883022374\n",
      "Epoch 7\n",
      "Batch 900\n",
      "Loss averaged over last 50 batches 0.144247816136\n",
      "Epoch 7\n",
      "Batch 950\n",
      "Loss averaged over last 50 batches 0.129608957913\n",
      "Epoch 7\n",
      "Batch 1000\n",
      "Loss averaged over last 50 batches 0.148356045583\n",
      "Epoch 7\n",
      "Batch 1050\n",
      "Loss averaged over last 50 batches 0.130716330809\n",
      "Epoch 7\n",
      "Batch 1100\n",
      "Loss averaged over last 50 batches 0.140530223175\n",
      "Epoch 7\n",
      "Batch 1150\n",
      "Loss averaged over last 50 batches 0.130272069862\n",
      "Epoch 8\n",
      "Batch 50\n",
      "Loss averaged over last 50 batches 0.276090665003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-e5e68f2b7b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0moneHotY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mneuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplayStep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moneHotY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# evaluate trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-8ec747ac2b99>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(self, nEpochs, learningRate, batchSize, X, y, displaySteps, oneHotY)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXBatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYBatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mbatchLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mrecentLoss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatchLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-8ec747ac2b99>\u001b[0m in \u001b[0;36mforwardProp\u001b[0;34m(self, XBatch)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# forward propagate through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-81f38ddbc6f6>\u001b[0m in \u001b[0;36mforwardProp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#print(\"From within fully connected layer's forwardProp:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#print(\"Type of Z_c:\", type(Z_c))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mA_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print(\"Type of A_c:\", type(A_c))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#print(\"From within fully connected layer's forwardProp:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-e455be72095b>\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(Z)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get data\n",
    "X_train, X_test, y_train, y_test = get_mnist_data()\n",
    "\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = y_train.reshape(-1,1), y_test.reshape(-1,1)\n",
    "\n",
    "#print('Shape of X_train:',X_train.shape)\n",
    "#print('Shape of y_train:',y_train.shape)\n",
    "\n",
    "# build network\n",
    "neuralNet = FFNetwork()\n",
    "\n",
    "n1 = 300\n",
    "dropoutRate1 = 0.5\n",
    "n2 = 300\n",
    "dropoutRate2 = 0.5\n",
    "n3 = 10\n",
    "\n",
    "neuralNet.addFCLayer(n1,activation='tanh')\n",
    "neuralNet.addDropoutLayer(dropoutRate1)\n",
    "neuralNet.addFCLayer(n2,activation='tanh')\n",
    "neuralNet.addDropoutLayer(dropoutRate2)\n",
    "neuralNet.addFCLayer(n3,activation='softmax')\n",
    "\n",
    "neuralNet.fixateNetwork(X_train[:10,:])\n",
    "\n",
    "print(neuralNet)\n",
    "\n",
    "# train network\n",
    "nEpochs = 10\n",
    "learningRate = 0.5\n",
    "batchSize = 50\n",
    "displayStep = 50\n",
    "oneHotY = True\n",
    "\n",
    "neuralNet.trainNetwork(nEpochs,learningRate,batchSize,X_train,y_train,displayStep,oneHotY)\n",
    "\n",
    "# evaluate trained model\n",
    "PTrain = neuralNet.predict(X_train)\n",
    "PTest = neuralNet.predict(X_test)\n",
    "\n",
    "accuracyTrain = np.sum(PTrain == y_train) / len(PTrain)\n",
    "accuracyTest = np.sum(PTest == y_test) / len(PTest)\n",
    "\n",
    "print('Accuracy on training set:',accuracyTrain)\n",
    "print('Accuracy on test set:',accuracyTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply network class to MNIST - build convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 784)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of X_train: (60000, 1, 28, 28)\n",
      "Shape of y_train: (60000, 1)\n",
      "\n",
      " \n",
      "----------------------------------------\n",
      " Layer type: (Secret) input layer\n",
      " Shape of input/output data (channels,height,width): [1, 32, 32]\n",
      "----------------------------------------\n",
      " Layer type: Convolution layer\n",
      " Shape of kernel (width,height): 5,5\n",
      " Stride used for kernel: 1\n",
      " Shape of input data (channels,height,width): [1, 32, 32]\n",
      " Padding used: valid\n",
      " Type of activation used: tanh\n",
      " Shape of output data (channels,height,width): [6, 28, 28]\n",
      "----------------------------------------\n",
      " Layer type: Pooling layer (max)\n",
      " Shape of pool (width,height): 2,2\n",
      " Stride used for pool: 2\n",
      " Shape of input data (channels,height,width): [6, 28, 28]\n",
      " Padding used: valid\n",
      " Shape of output data (channels,height,width): [6, 14, 14]\n",
      "----------------------------------------\n",
      " Layer type: Convolution layer\n",
      " Shape of kernel (width,height): 5,5\n",
      " Stride used for kernel: 1\n",
      " Shape of input data (channels,height,width): [6, 14, 14]\n",
      " Padding used: valid\n",
      " Type of activation used: tanh\n",
      " Shape of output data (channels,height,width): [16, 10, 10]\n",
      "----------------------------------------\n",
      " Layer type: Pooling layer (max)\n",
      " Shape of pool (width,height): 2,2\n",
      " Stride used for pool: 2\n",
      " Shape of input data (channels,height,width): [16, 10, 10]\n",
      " Padding used: valid\n",
      " Shape of output data (channels,height,width): [16, 5, 5]\n",
      "----------------------------------------\n",
      " Layer type: Reshaping layer (Convolution -> Fully connected)\n",
      " Shape of input data: [16, 5, 5]\n",
      " Shape of output data: [400]\n",
      "----------------------------------------\n",
      " Layer type: Fully connected layer\n",
      " Number of neurons in input data: [400]\n",
      " Type of activation used: tanh\n",
      " Number of neurons in output data: [84]\n",
      "----------------------------------------\n",
      " Layer type: Fully connected layer\n",
      " Number of neurons in input data: [84]\n",
      " Type of activation used: softmax\n",
      " Number of neurons in output data: [10]\n",
      "Epoch 0\n",
      "Batch 10\n",
      "Loss averaged over last 10 batches 2.34834498004\n",
      "Epoch 0\n",
      "Batch 20\n",
      "Loss averaged over last 10 batches 2.30329263225\n",
      "Epoch 0\n",
      "Batch 30\n",
      "Loss averaged over last 10 batches 2.19457450539\n",
      "Epoch 0\n",
      "Batch 40\n",
      "Loss averaged over last 10 batches 1.71923369987\n",
      "Epoch 0\n",
      "Batch 50\n",
      "Loss averaged over last 10 batches 0.801274415212\n",
      "Epoch 0\n",
      "Batch 60\n",
      "Loss averaged over last 10 batches 0.738253216152\n",
      "Epoch 0\n",
      "Batch 70\n",
      "Loss averaged over last 10 batches 0.589865348069\n",
      "Epoch 0\n",
      "Batch 80\n",
      "Loss averaged over last 10 batches 0.557005429124\n",
      "Epoch 0\n",
      "Batch 90\n",
      "Loss averaged over last 10 batches 0.447712360304\n",
      "Epoch 0\n",
      "Batch 100\n",
      "Loss averaged over last 10 batches 0.606892601436\n",
      "Epoch 0\n",
      "Batch 110\n",
      "Loss averaged over last 10 batches 0.425383032421\n",
      "Epoch 0\n",
      "Batch 120\n",
      "Loss averaged over last 10 batches 0.404482218741\n",
      "Epoch 0\n",
      "Batch 130\n",
      "Loss averaged over last 10 batches 0.519970278661\n",
      "Epoch 0\n",
      "Batch 140\n",
      "Loss averaged over last 10 batches 0.38659828048\n",
      "Epoch 0\n",
      "Batch 150\n",
      "Loss averaged over last 10 batches 0.35984660418\n",
      "Epoch 0\n",
      "Batch 160\n",
      "Loss averaged over last 10 batches 0.453530665186\n",
      "Epoch 0\n",
      "Batch 170\n",
      "Loss averaged over last 10 batches 0.318521824138\n",
      "Epoch 0\n",
      "Batch 180\n",
      "Loss averaged over last 10 batches 0.305709509774\n",
      "Epoch 0\n",
      "Batch 190\n",
      "Loss averaged over last 10 batches 0.353786349482\n",
      "Epoch 0\n",
      "Batch 200\n",
      "Loss averaged over last 10 batches 0.220439329383\n",
      "Epoch 0\n",
      "Batch 210\n",
      "Loss averaged over last 10 batches 0.273082290911\n",
      "Epoch 0\n",
      "Batch 220\n",
      "Loss averaged over last 10 batches 0.353416215891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-16d692ba9d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0moneHotY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mneuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplayStep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moneHotY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-22a7c0679549>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(self, nEpochs, learningRate, batchSize, X, y, displaySteps, oneHotY)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXBatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYBatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mbatchLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mrecentLoss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatchLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-22a7c0679549>\u001b[0m in \u001b[0;36mforwardProp\u001b[0;34m(self, XBatch)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# forward propagate through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-5178f393f104>\u001b[0m in \u001b[0;36mforwardProp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mhStart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhEnd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwStart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetConvSliceCorners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mX_hw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhStart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhEnd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwStart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwEnd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mY_hw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_hw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mZ_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_hw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get data\n",
    "X_train, X_test, y_train, y_test = get_mnist_data()\n",
    "\n",
    "print('Shape of X_train:',X_train.shape)\n",
    "print('Shape of y_train:',y_train.shape)\n",
    "\n",
    "X_train, X_test = np.array(X_train).reshape(-1,1,28,28),np.array(X_test).reshape(-1,1,28,28)\n",
    "y_train, y_test = y_train.reshape(-1,1), y_test.reshape(-1,1)\n",
    "\n",
    "print('Shape of X_train:',X_train.shape)\n",
    "print('Shape of y_train:',y_train.shape)\n",
    "\n",
    "# build network\n",
    "neuralNet = FFNetwork(2)\n",
    "\n",
    "kernelSize1 = 5\n",
    "channels1 = 6\n",
    "stride1 = 1\n",
    "padding1 = 'valid'\n",
    "\n",
    "poolingSize2 = 2\n",
    "stride2 = 2\n",
    "padding2 = 'valid'\n",
    "\n",
    "kernelSize3 = 5\n",
    "channels3 = 16\n",
    "stride3 = 1\n",
    "padding3 = 'valid'\n",
    "\n",
    "poolingSize4 = 2\n",
    "stride4 = 2\n",
    "padding4 = 'valid'\n",
    "\n",
    "n5 = 400\n",
    "\n",
    "n6 = 84\n",
    "\n",
    "n7 = 10\n",
    "\n",
    "neuralNet.addConvLayer(kernelHeight=kernelSize1,\n",
    "                       kernelWidth=kernelSize1,\n",
    "                       channels=channels1,\n",
    "                       stride=stride1,\n",
    "                       padding=padding1,\n",
    "                       activation='tanh')\n",
    "neuralNet.addPoolingLayer(poolingHeight=poolingSize2,\n",
    "                       poolingWidth=poolingSize2,\n",
    "                       stride=stride2,\n",
    "                       padding=padding2,\n",
    "                       poolingType='max')\n",
    "neuralNet.addConvLayer(kernelHeight=kernelSize3,\n",
    "                       kernelWidth=kernelSize3,\n",
    "                       channels=channels3,\n",
    "                       stride=stride3,\n",
    "                       padding=padding3,\n",
    "                       activation='tanh')\n",
    "neuralNet.addPoolingLayer(poolingHeight=poolingSize4,\n",
    "                       poolingWidth=poolingSize4,\n",
    "                       stride=stride4,\n",
    "                       padding=padding4,\n",
    "                       poolingType='max')\n",
    "\n",
    "neuralNet.addConvToFCReshapeLayer(n5)\n",
    "\n",
    "neuralNet.addFCLayer(n6,activation='tanh')\n",
    "\n",
    "neuralNet.addFCLayer(n7,activation='softmax')\n",
    "\n",
    "neuralNet.fixateNetwork(X_train[:10,:,:,:])\n",
    "\n",
    "print('\\n ')\n",
    "print(neuralNet)\n",
    "\n",
    "# train network\n",
    "nEpochs = 1\n",
    "learningRate = 0.5\n",
    "batchSize = 50\n",
    "displayStep = 10\n",
    "oneHotY = True\n",
    "\n",
    "neuralNet.trainNetwork(nEpochs,learningRate,batchSize,X_train,y_train,displayStep,oneHotY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on subset of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9462\n",
      "Accuracy on test set: 0.9195\n"
     ]
    }
   ],
   "source": [
    "# evaluate trained model\n",
    "PTrain = neuralNet.predict(X_train[:5000])\n",
    "PTest = neuralNet.predict(X_test[:2000])\n",
    "\n",
    "accuracyTrain = np.sum(PTrain == y_train[:5000]) / len(PTrain)\n",
    "accuracyTest = np.sum(PTest == y_test[:2000]) / len(PTest)\n",
    "\n",
    "print('Accuracy on training set:',accuracyTrain)\n",
    "print('Accuracy on test set:',accuracyTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 3421, 412]|[12, 352, 121]\n"
     ]
    }
   ],
   "source": [
    "a = [22, 3421,412]\n",
    "b = [12,352,121]\n",
    "print('|'.join([str(a),str(b)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
